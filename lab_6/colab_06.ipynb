{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Example: The Jogger\n",
    "\n",
    "### Going Jogging\n",
    "In this notebook, we will explore reinforcement learning in a small, simple state space: Our agent starts at home (left), wants to jog to the target (right), and make it back home. When reaching\n",
    "this goal, the agent is rewarded a large reward R and goes to sleep, terminating our game in state BED. In each time step, the agent can move one to the left, one to the right, or stay where it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"illustration.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Challenge: Rain\n",
    "The tricky thing is: When not at home, the agent risks getting wet in the rain. This is modeled by a chance of\n",
    "rain between 0% and 100% (the blue bar in the plot). This chance changes with each time step by\n",
    "±10%. Whenever the chance is 100%, it rains. When the agent is outside while raining, it gets a\n",
    "large negative reward −R and the process terminates in state WET."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation\n",
    "We download some code for plotting our state spaces..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install networkx\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What's in an RL Environment?\n",
    "This interface defines what we would require from a reinforcement learning environment: Basically, a set of states and transitions between them, possible actions, and rewards.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    '''\n",
    "    interface for modelling an RL environment.\n",
    "    '''\n",
    "    \n",
    "    def states(self):\n",
    "        '''returns the environment's states as a list of tuples.'''\n",
    "        raise NotImplementedError()\n",
    "  \n",
    "    def is_final(self, state):\n",
    "        '''returns True iff. a state is terminal.'''\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def actions(self):\n",
    "        '''returns the environment's actions as a list.'''\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def start(self):\n",
    "        '''returns an initial state.'''\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def transitions(self):\n",
    "        '''the state transition model P. Returns a nested dictionary\n",
    "           in which trans[s][a][s'] is the probability that \n",
    "           picking action a in state s leads to the state s'. '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def reward(self, s, a=None):\n",
    "        '''returns a reward (float), depending on the current state and\n",
    "           (optionally) action.'''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Our Jogging Environment\n",
    "\n",
    "Let's think about our environment:\n",
    "\n",
    "* What do we need to store in a *'state'*?\n",
    "* Let's say that T is the number of steps from house to target. *How many* states are there, roughly?\n",
    "* What does the *transition structure* of our state space look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"illustration.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "############           NO NEED TO TOUCH THIS      #######################\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "class JoggingEnvironment(Environment):\n",
    "    '''\n",
    "    environment for our 'jogging' problem.\n",
    "    \n",
    "    In this environment, each STATE is a triple (position, rain_chance, reached_target),\n",
    "    where:\n",
    "    \n",
    "        o position in {0,1,2,...,T}\n",
    "\n",
    "          where pos=0 means being at home, and pos=T means being\n",
    "          at the target. The agent's goal is to arrive back home \n",
    "          after having reached T at least once.\n",
    "\n",
    "        o rain_chance in {0%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 100%}\n",
    "\n",
    "          rain_chance describes the current weather. The value changes +-10%\n",
    "          with each time step. When at 100%, it rains. If the agent is not at\n",
    "          home in the rain, it looses.\n",
    "\n",
    "        o reached_target in {0,1}.\n",
    "\n",
    "          reached_target indicates whether the agent has reached the \n",
    "          target and is on its way back (1) or not (0).\n",
    "        \n",
    "        There are two special terminal states: WET (failure, (-1,-1,-1)) and \n",
    "                                               BED (success, (-2,-2,-2)).\n",
    "\n",
    "    '''\n",
    "    def __init__(self, T=9, R=1000):\n",
    "        '''\n",
    "        class constructor.\n",
    "\n",
    "        @param T: the position of the target (must be a positive integer)\n",
    "        @type T: int\n",
    "        @param R: the reward for reaching BED (-R when getting WET).\n",
    "        @type R: int\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.T = T\n",
    "        self.R = R\n",
    "\n",
    "        self.WET = (-1,-1,-1)   # terminal state for failure\n",
    "        self.BED = (-2,-2,-2)   # terminal state for success\n",
    "\n",
    "        self._states = None\n",
    "        self._actions = None\n",
    "        self._transitions = None\n",
    "\n",
    "\n",
    "    def states(self):\n",
    "        '''\n",
    "        returns the states of the RL environment. \n",
    "\n",
    "        @rtype: set( tuple )\n",
    "        @return: the states. \n",
    "\n",
    "        '''\n",
    "        if self._states is None:\n",
    "            \n",
    "            self._states = {self.WET, self.BED}\n",
    "        \n",
    "            for position in range(0,self.T+1):\n",
    "                for rain_chance in range(0,101,10):\n",
    "                    for reached_target in [0,1]:\n",
    "                        # raining and not at home -> WET\n",
    "                        if rain_chance==100 and position>0:\n",
    "                            continue\n",
    "                        # at home after reaching target -> BED\n",
    "                        if reached_target==1 and position==0:\n",
    "                            continue\n",
    "                        # at target -> reached_target=1\n",
    "                        if position==self.T and reached_target==0:\n",
    "                            continue\n",
    "\n",
    "                        self._states.add((position, rain_chance, reached_target))\n",
    "\n",
    "        return self._states\n",
    "\n",
    "\n",
    "    def is_final(self, state):\n",
    "        '''return true iff. in terminal state (WET/BED).'''\n",
    "        return state==self.WET or state==self.BED  \n",
    "\n",
    "\n",
    "    def start(self):\n",
    "        '''jogger starts at home, at random weather.'''\n",
    "        rain_chance = random.choice(range(50,101,10))\n",
    "        return (0,rain_chance,0)\n",
    "\n",
    "    \n",
    "    def actions(self):\n",
    "        '''\n",
    "        As an ACTION, the agent can either ...\n",
    "        o a= 0 : ... stay at the current position \n",
    "        o a=-1 : ... run 1 step 'left', towards home.\n",
    "        o a= 1 : ... run 1 step 'right', towards the target.\n",
    "        '''\n",
    "        if self._actions is None:\n",
    "            self._actions = {-1, 0, 1}\n",
    "        return self._actions\n",
    "\n",
    "    \n",
    "    def reward(self, s, a=None):\n",
    "        '''\n",
    "        returns the reward, depending on the current state \n",
    "        (the action is neglected).\n",
    "\n",
    "        @rtype: float\n",
    "        @return: the reward. Each step costs -1. When in BED, \n",
    "                 terminate with reward R. When in WET, terminate \n",
    "                 with reward -R.\n",
    "        '''\n",
    "        if s==self.BED:\n",
    "            return self.R\n",
    "        elif s==self.WET:\n",
    "            return -self.R\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "\n",
    "    def _compute_transitions(self):\n",
    "        '''\n",
    "        The transition model: For each (s,a) pair and each possible\n",
    "        follow-up rain chance, compute the follow-up probability.\n",
    "\n",
    "        @rtype: dict ( dict ( dict ( float ) ) )\n",
    "        @return: in the result dictionary, \n",
    "                 transitions[s][a][s'] is the probability that --\n",
    "                 when the agent is in state s and picks action a --\n",
    "                 the state switches to s'.\n",
    "        '''\n",
    "        transitions = {}\n",
    "        \n",
    "        for s in self.states():\n",
    "        \n",
    "            position,rain_chance,reached_target = s\n",
    "\n",
    "            transitions[s] = {}\n",
    "\n",
    "            if self.is_final(s):\n",
    "                continue\n",
    "\n",
    "            for a in self.actions():\n",
    "\n",
    "                transitions[s][a] = {}\n",
    "\n",
    "                # move -1/0/+1. Cannot move to <0 or >T.\n",
    "                new_position = min( self.T, max(0, position + a ) )\n",
    "                new_reached_target = reached_target or int( new_position == self.T )\n",
    "\n",
    "                if new_reached_target and new_position==0:\n",
    "\n",
    "                    # reached BED\n",
    "                    transitions[s][a][self.BED] = 1.0\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # change rain chance -10/0/+10. Cannot be <0 or >100.\n",
    "                    for new_rain_chance in [max(0, rain_chance - 10),\n",
    "                                            rain_chance,\n",
    "                                            min(100, rain_chance + 10)]:\n",
    "\n",
    "                        # reached WET\n",
    "                        if new_position > 0 and new_rain_chance == 100:\n",
    "                            news = self.WET\n",
    "                        else:\n",
    "                            news = (new_position,new_rain_chance,new_reached_target)\n",
    "\n",
    "                        # add 33% probability for new state.\n",
    "                        transitions[s][a][news] = transitions[s][a].get(news, 0) + 1.0/3.0\n",
    "                            \n",
    "        return transitions\n",
    "\n",
    "    \n",
    "    def transitions(self,):\n",
    "        if self._transitions is None:\n",
    "            self._transitions = self._compute_transitions()\n",
    "        return self._transitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspecting our State Space\n",
    "I have added some (ugly) code to visualize our state space as a graph, with the states as nodes and the transitions between them as edges.\n",
    "\n",
    "We can now create an environment, check how many states we obtain, and visualize the state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "############           NO NEED TO TOUCH THIS      #######################\n",
    "#########################################################################\n",
    "\n",
    "# setup figure \n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "class StateSpaceVisualizer():\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self._build_graph(env)\n",
    "        \n",
    "        \n",
    "    def _build_graph(self, env):\n",
    "        \n",
    "        self.states = sorted(env.states())\n",
    "        self.int2state = dict(enumerate(self.states))\n",
    "        self.state2int = {v:k for (k,v) in self.int2state.items()}\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # building nodes (including their positions)\n",
    "        for s in self.states:\n",
    "            (position, rain_chance, reached_target) = s\n",
    "            if s == env.WET:\n",
    "                pos = (env.T*1.25, 11)\n",
    "            elif s == env.BED:\n",
    "                pos = (env.T*2.5, 5)\n",
    "            elif not reached_target:\n",
    "                pos = (position,rain_chance/10)\n",
    "            else:\n",
    "                pos = (env.T*2.5 - position,rain_chance/10)\n",
    "            G.add_node(self.state2int[s], label=str(s), pos=pos)\n",
    "\n",
    "        # building edges\n",
    "        for s in self.states:\n",
    "            for a in env.transitions()[s]:\n",
    "                for s2 in env.transitions()[s][a]:\n",
    "                    G.add_edge(self.state2int[s],self.state2int[s2],label=a)        \n",
    "        \n",
    "        \n",
    "        # build a plotly object for the edges\n",
    "        edge_x,edge_y = [], []\n",
    "        for edge in G.edges():\n",
    "            x0, y0 = G.nodes[edge[0]]['pos']\n",
    "            x1, y1 = G.nodes[edge[1]]['pos']\n",
    "            edge_x.append(x0)\n",
    "            edge_x.append(x1)\n",
    "            edge_x.append(None)\n",
    "            edge_y.append(y0)\n",
    "            edge_y.append(y1)\n",
    "            edge_y.append(None)\n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line=dict(width=0.5, color='#888'),\n",
    "            hoverinfo='none',\n",
    "            mode='lines')\n",
    "\n",
    "        # build a plotly object for the nodes\n",
    "        node_x = []\n",
    "        node_y = []\n",
    "        node_text = []\n",
    "        for node in G.nodes():\n",
    "            x, y = G.nodes[node]['pos']\n",
    "            node_x.append(x)\n",
    "            node_y.append(y)\n",
    "            state = self.int2state[node]\n",
    "            if state == env.WET:\n",
    "                state = 'WET (FAILURE)'\n",
    "            elif state == env.BED:\n",
    "                state = 'BED (SUCCESS)'\n",
    "            node_text.append(str(state))\n",
    "        node_trace = go.Scatter(\n",
    "            x=node_x, y=node_y,\n",
    "            mode='markers',\n",
    "            hoverinfo='text',\n",
    "            marker=dict(\n",
    "                showscale=True,\n",
    "                colorscale='YlGnBu',\n",
    "                reversescale=True,\n",
    "                color=[],\n",
    "                size=10,\n",
    "                colorbar=dict(\n",
    "                    thickness=15,\n",
    "                    title='value v(s)',\n",
    "                    xanchor='left',\n",
    "                    titleside='right'\n",
    "                ),\n",
    "                line_width=2))\n",
    "        node_trace.text = node_text\n",
    "\n",
    "        # build the overall FigureWidget (FigureWidets are updateable)\n",
    "        self.fig = go.FigureWidget(data=[edge_trace, node_trace],\n",
    "                     layout=go.Layout(\n",
    "                        title='Iteration 0',\n",
    "                        titlefont_size=16,\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20,l=5,r=5,t=40),\n",
    "                        annotations=[],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "                        )\n",
    "        \n",
    "        # show the figure\n",
    "        display(self.fig)\n",
    "\n",
    "        self.G = G\n",
    "\n",
    "        \n",
    "    def action2str(self, best_actions, s):\n",
    "        a = best_actions.get(s, 0)\n",
    "        if a==-1:\n",
    "            return '<--'\n",
    "        elif a==1:\n",
    "            return '-->'\n",
    "        else:\n",
    "            return '--'\n",
    "        \n",
    "    def update_node_values(self, v, best_actions):\n",
    "        values_list = [v[s] for s in self.states]\n",
    "        self.fig.data[1]['marker']['color'] = values_list\n",
    "        self.fig.data[1]['text'] = ['%s: value=%.1f, action=%s' %(str(s), v[s], self.action2str(best_actions, s))\n",
    "                                    for s in self.states]\n",
    "        \n",
    "        # clear display output and show updated figure\n",
    "        display.clear_output(wait=True)\n",
    "        self.fig.show()\n",
    "        \n",
    "    def update_title(self, title):\n",
    "        self.fig.layout.title = title\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = JoggingEnvironment(T=6)\n",
    "print('Our environment contains', len(env.states()), 'states.')\n",
    "print(env.states())\n",
    "vis = StateSpaceVisualizer(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Inspecting values\n",
    "If we have the time, maybe we can think about some states' value functions. Can you compute these...:\n",
    "* $v_*(1,80,1)$\n",
    "* $v_*(6,20,1)$\n",
    "* $v_*(6,50,1)$...?\n",
    "\n",
    "We assume we follow an optimal policy, a reward of $R=100$, and a discount factor of $\\gamma=1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Getting Our Hands Dirty: Let's Play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"code.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now code a method **play()**, which runs the game once:\n",
    "* Our method should generate a **sequence of states**, which ends either in state BED or in state WET.\n",
    "* To do so, our method  get a **policy** (i.e., the agent's strategy).\n",
    "* Our policy is **stochastic**: In each state, it will pick actions with certain probabilities. We model it as a dictionary with states as keys. For each state s, policy[s] is a list of action-probability pairs, e.g. [(left,0.25), (right,0.5), (stay,0.25)].\n",
    "\n",
    "In the next cell, ...\n",
    "1. implement play()\n",
    "2. define a dummy policy which just runs to the target and back, regardless of the weather.\n",
    "3. play the game and observe the resulting sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import multinomial\n",
    "\n",
    "def choose_transition(s,a,env):\n",
    "    ''' when picking action a in state s, compute a random follow-up\n",
    "        state s' according to the environment's transition model. ''' \n",
    "    next_states = list(env.transitions()[s][a].items()) # (state,prob) pairs\n",
    "    probs = [prob for a,prob in next_states]\n",
    "    selected = int(np.where(multinomial.rvs(1, p=probs))[0])\n",
    "    return next_states[selected][0]\n",
    "    \n",
    "def choose_action(policy, s):\n",
    "    ''' when in a state s, choose the next action a according to \n",
    "        the policy.'''\n",
    "    probs = [prob for a,prob in policy[s]]\n",
    "    selected = int(np.where(multinomial.rvs(1, p=probs))[0])\n",
    "    return policy[s][selected][0]\n",
    "    \n",
    "def play(env, policy, start=None):\n",
    "    ''' Given an environment and a policy, run the agent.\n",
    "        Return the resulting state sequence as a list of states/tuples. '''\n",
    "    s = start if start else env.start()\n",
    "    seq = [s]\n",
    "    while not env.is_final(s):\n",
    "        a = choose_action(policy,s)      \n",
    "        s = choose_transition(s,a,env)\n",
    "        seq.append(s)\n",
    "    return seq\n",
    "\n",
    "        \n",
    "def dummy_policy():\n",
    "    always_right = [(-1,0),(1,1),(0,0)]\n",
    "    always_left = [(-1,1),(1,0),(0,0)]\n",
    "    def _dummy_action(s):\n",
    "        return always_left if s[-1] else always_right\n",
    "    return { s:_dummy_action(s) for s in env.states() }\n",
    "\n",
    "# visualizing episodes\n",
    "policy = dummy_policy()\n",
    "\n",
    "for i in range(1):\n",
    "    seq = play(env,policy)\n",
    "    print(seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Game\n",
    "I have prepared another visualizer object, which gives us an animation of a play...\n",
    "In the cell below, you can generate state sequences with play() and then visualize them. The blue timeline on the left illustrates the recent rain chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "############           NO NEED TO TOUCH THIS      #######################\n",
    "#########################################################################\n",
    "\n",
    "import plotly\n",
    "import matplotlib.image as image\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "\n",
    "# import display function\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "class RunVisualizer:\n",
    "    \n",
    "    def __init__(self, env, sleep=0.5):\n",
    "        self.sleep = sleep\n",
    "        self.env = env\n",
    "        self.T = env.T\n",
    "        self.fig = go.FigureWidget()\n",
    "        self.fig.update_yaxes(range=[0, 1])\n",
    "        self.fig.update_xaxes(range=[-1.5, self.T+1-0.5])\n",
    "        # add background image\n",
    "        self.fig.add_layout_image(\n",
    "            dict(\n",
    "                source='https://raw.githubusercontent.com/HSG-AIML-Teaching/ML2022-Lab/main/lab_6/background.jpg',\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x=-0.5,\n",
    "                y=1,\n",
    "                sizex=self.T+1,\n",
    "                sizey=1,\n",
    "                sizing=\"stretch\",\n",
    "                opacity=1,\n",
    "                layer=\"below\")\n",
    "        )\n",
    "        # add scatter plot\n",
    "        self.fig.add_trace(\n",
    "            go.Scatter(x=[-1, -0.5, 0], y=[0.5, 0.2, 0.8]))\n",
    "        # load image of jogger to display\n",
    "        img_right = image.imread('https://raw.githubusercontent.com/HSG-AIML-Teaching/ML2022-Lab/main/lab_6/jogger_small.png')\n",
    "        img_right[:,:,:3] *= 255                 # range [0,1]->[0,255]\n",
    "        img_right[:,:,:] = img_right[::-1,:,:]   # flip vertically\n",
    "        img_left = img_right[:,::-1,:]           # a second image of the jogger running left.\n",
    "        self.img_right = img_right\n",
    "        self.img_left = img_left\n",
    "        # add jogger image to plot\n",
    "        self.fig.add_trace(go.Image(z=img_right, colormodel='rgba', x0=3, y0=.1, dx=0.0061, dy=0.0061))\n",
    "        # display(self.fig)\n",
    "        \n",
    "        # show figure\n",
    "        self.fig.show()\n",
    "        \n",
    "    def next_state(self, i, seq):\n",
    "        state = seq[i]\n",
    "        pos,rain_chance,reached_target = state\n",
    "        # set barplot\n",
    "        y = np.array([s[1] for j,s in enumerate(seq) if j<=i])\n",
    "        if state == self.env.WET:\n",
    "            y[-1] = 100.\n",
    "        elif state == self.env.BED:\n",
    "            y[-1] = y[-2]\n",
    "        y = (y/100.)[-5:]\n",
    "        y = np.clip(y, 0.03, 0.97)\n",
    "        #y = [v/100. for v in [s[1] for s in seq][:-5]]\n",
    "        self.fig.data[0]['y'] = y\n",
    "        self.fig.data[0]['x'] = np.arange(-1.5,-1.5+0.2*len(y),0.2)\n",
    "        # set position\n",
    "        if state == self.env.BED:\n",
    "            pos = 0\n",
    "        elif state == self.env.WET:\n",
    "            pos = -100\n",
    "        self.fig.data[1]['x0'] = pos-0.3\n",
    "        # set jogger image (left/right)\n",
    "        self.fig.data[1]['z'] = self.img_left if reached_target else self.img_right\n",
    "        time.sleep(self.sleep)\n",
    "        \n",
    "        # clear display output and show figure\n",
    "        display.clear_output(wait=True)\n",
    "        self.fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(seq,env,vis):\n",
    "    for i,s in enumerate(seq):\n",
    "        vis.next_state(i, seq)\n",
    "\n",
    "env = JoggingEnvironment(T=5)\n",
    "vis = RunVisualizer(env=env, sleep=1.00)\n",
    "policy = dummy_policy()\n",
    "\n",
    "for i in range(10):\n",
    "    seq = play(env,policy)\n",
    "    plot(seq, env, vis)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training with Dynamic Programming\n",
    "Let us now implement the training of a better agent, using the dynamic programming approach from lecture:\n",
    "* The agent starts with a totally random policy (i.e., an $\\epsilon$-greedy policy with $\\epsilon$=1).\n",
    "* We alternative between (a) updating the states' value functions (**policy evaluation**), and (b) updating the policy by lowering $\\epsilon$ (**policy improvement**).\n",
    "\n",
    "There is already a bit of code which defines the epsilon-greedy policy and chooses the best action from it. Let's implement the training loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NITERS_DYNAMIC = 50\n",
    "NITERS_MONTE_CARLO = 5\n",
    "NITERS_POLICY_ITERATION = 40\n",
    "\n",
    "############### HELP METHODS ##########################\n",
    "        \n",
    "        \n",
    "def best_action(s, values, transitions):\n",
    "    '''given a state and the value function, compute the best action\n",
    "       according to the Bellman Equation.'''\n",
    "    q = {}\n",
    "    for a in transitions[s]:\n",
    "        q[a] = sum([Pnews*values[news] for news,Pnews in transitions[s][a].items()])\n",
    "        #if s==(1,0,1):\n",
    "        #    print('???', a)\n",
    "        #    for news,Pnews in transitions[s][a].items():\n",
    "        #        print(' -> news =', news, 'P =', Pnews, 'V =', values[news])\n",
    "    besta,bestq = sorted(q.items(), key=lambda x:x[-1], reverse=True)[0]\n",
    "    return besta\n",
    "\n",
    "def eps_greedy_policy(v, eps, env):\n",
    "    '''given a value function, compute the epsilon-greedy policy.\n",
    "       policy[s] is a list of (a,P) pairs where a is an action and\n",
    "       P is its probability of picking the action according to the policy.'''\n",
    "    policy = {}\n",
    "    best_actions = {}\n",
    "    for s in env.states():\n",
    "        if not env.transitions()[s]:\n",
    "            continue\n",
    "        valid_actions = [a for a in env.actions() if a in env.transitions()[s]]\n",
    "        P = 1./len(valid_actions)\n",
    "        if eps==1:\n",
    "            policy[s] = [(a,P) for a in valid_actions]\n",
    "        else:\n",
    "            abest = best_action(s,v,env.transitions())\n",
    "            policy[s] = [(a,eps*P + (1-eps)*(a==abest)) for a in valid_actions]\n",
    "            best_actions[s] = abest\n",
    "            #if s==(1,0,1):\n",
    "            #    print('valid_actions', valid_actions)\n",
    "            #    print('best action', abest)\n",
    "    return policy, best_actions\n",
    "\n",
    "\n",
    "\n",
    "############### ACTUAL TRAINING LOOP ##########################\n",
    "\n",
    "def evaluate_policy_dynamic_programming(env, policy, gamma):\n",
    "\n",
    "    states = env.states()\n",
    "    transitions = env.transitions()\n",
    "    \n",
    "    # initialize value function\n",
    "    v = {s:0. for s in states}\n",
    "    \n",
    "    # training iterations\n",
    "    for i in range(NITERS_DYNAMIC):\n",
    "\n",
    "        newv = {}\n",
    "    \n",
    "        for s in states:\n",
    "            r = env.reward(s)\n",
    "            v_ = 0.\n",
    "            if transitions[s]: # -> s is not terminal\n",
    "                for (a,Pa) in policy[s]:\n",
    "                    v_ += Pa * ( sum([Pnews*v[news]\n",
    "                                      for news,Pnews\n",
    "                                      in transitions[s][a].items()]) )\n",
    "                #if s == (1,0,1):\n",
    "                #    print(i, '> Evaluating', s)\n",
    "                #    for (a,Pa) in policy[s]:\n",
    "                #        print('action', a)\n",
    "                #        for news,Pnews in transitions[s][a].items():\n",
    "                #            print(news, '(', Pnews, ') : ', v[news])\n",
    "                #    print('> RESULT =', v_, r + gamma*v_)\n",
    "                \n",
    "            newv[s] = r + gamma * v_\n",
    "    \n",
    "        v = newv\n",
    "    \n",
    "    return v\n",
    "\n",
    "\n",
    "def evaluate_policy_monte_carlo(env, policy, gamma, v, beta):\n",
    "    '''this updates v using monte carlo sampling.'''\n",
    "    states = env.states()\n",
    "\n",
    "    for i in range(NITERS_MONTE_CARLO):\n",
    "        \n",
    "        for start in states:\n",
    "            \n",
    "            episode = play(env, policy, start=start)\n",
    "            \n",
    "            v_est = 0.\n",
    "            for s in episode[::-1]:\n",
    "                v_est = env.reward(s) + gamma * v_est\n",
    "                v[s] = v[s] + beta * (v_est-v[s])\n",
    "                \n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "def train(env, iterations, eps_factor, use_monte_carlo=False, eps0=1., gamma=0.5, visualizer=None, beta=0.02):\n",
    "    \n",
    "    v = {s:0. for s in env.states()}\n",
    "    eps = eps0\n",
    "    eps_policy,_ = eps_greedy_policy(v, eps, env)\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # policy evaluation\n",
    "        if use_monte_carlo:\n",
    "            v = evaluate_policy_monte_carlo(env, eps_policy, gamma, v, beta)\n",
    "        else:\n",
    "            v = evaluate_policy_dynamic_programming(env, eps_policy, gamma)\n",
    "        \n",
    "        # policy optimization\n",
    "        eps_policy, best_actions = eps_greedy_policy(v, eps, env)\n",
    "        #print(eps_policy)\n",
    "        result_policy = eps_greedy_policy(v, eps=0., env=env)\n",
    "\n",
    "        if visualizer and i<10 or i % 10 == 0:\n",
    "            visualizer.update_title('Iteration %d (eps=%.2f)' %(i,eps))\n",
    "            visualizer.update_node_values(v, best_actions)\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        if i==0 or i==99:\n",
    "            pass\n",
    "            #print(i)\n",
    "            #print_policy(policy)\n",
    "\n",
    "        eps *= eps_factor\n",
    "    \n",
    "    return result_policy, v\n",
    "\n",
    "vis = StateSpaceVisualizer(env)\n",
    "\n",
    "policy = train(env, iterations=300, use_monte_carlo=True, eps_factor=0.99, eps0=1.0, gamma=0.99, visualizer=vis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
